
@inproceedings{pham_mtsad_2019,
	title = {{MTSAD}: {Multivariate} {Time} {Series} {Abnormality} {Detection} and {Visualization}},
	shorttitle = {{MTSAD}},
	url = {https://ieeexplore.ieee.org/abstract/document/9006559},
	doi = {10.1109/BigData47090.2019.9006559},
	abstract = {Detecting outliers is one of the fundamental tasks in visual analytics and valuable in many application domains, such as suspicious network cyberattack recognition. This paper introduces an approach to analyzing and visualizing high-dimensional time series, focusing on identifying multivariate observations that are significantly different from the others. We also propose a prototype, called MTSAD, to guide users when interactively exploring abnormalities in large time series. The prototype contains two views: the main window provides an overview of identified outliers overtime, the detail window investigates and explores the ranked temporal data entries based on their outlying contributions to the overall plots. The visual interface supports a full range of interactions, such as lensing, brushing and linking, ranking, and filtering. To validate the benefits and usefulness of our approach, we demonstrate MTSAD on real-world datasets of different numbers of attributes.},
	urldate = {2023-10-15},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Pham, Vung and Nguyen, Ngan and Li, Jie and Hass, Jon and Chen, Yong and Dang, Tommy},
	month = dec,
	year = {2019},
	pages = {3267--3276},
	file = {IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/7MRMZ63Y/Pham et al. - 2019 - MTSAD Multivariate Time Series Abnormality Detect.pdf:application/pdf},
}

@inproceedings{li_monster_2020,
	title = {{MonSTer}: {An} {Out}-of-the-{Box} {Monitoring} {Tool} for {High} {Performance} {Computing} {Systems}},
	shorttitle = {{MonSTer}},
	url = {https://ieeexplore.ieee.org/abstract/document/9229641},
	doi = {10.1109/CLUSTER49012.2020.00022},
	abstract = {Understanding the status of high-performance computing platforms and correlating applications to resource usage provide insight into the interactions among platform components. A lot of efforts have been devoted into developing monitoring solutions; however, a large-scale HPC system usually requires a combination of methods/tools to successfully monitor all metrics, which will lead to a huge effort in configuration and monitoring. Besides, monitoring tools are often left behind in the procurement of large-scale HPC systems. These challenges have motivated the development of a next-generation out-of-the-box monitoring tool that can be easily deployed without losing informative metrics. In this work, we introduce MonSTer, an “out-of-the-box” monitoring tool for high-performance computing platforms. MonSTer uses the evolving specification Redfish to retrieve sensor data from Baseboard Management Controller (BMC), and resource management tools such as Univa Grid Engine (UGE) or Slurm to obtain application information and resource usage data. Additionally, it also uses a time-series database (e.g. InfluxDB) for data storage. MonSTer correlates applications to resource usage and reveals insightful knowledge without having additional overhead on the application and computing nodes. This paper presents the design and implementation of MonSTer, as well as experiences gained through real-world deployment on the 467-node Quanah cluster at Texas Tech University's High Performance Computing Center (HPCC) over the past year. In this work, we introduce MonSTer, an “out-of-the-box” monitoring tool for high-performance computing platforms. MonSTer uses the evolving specification Redfish to retrieve sensor data from Baseboard Management Controller (BMC), and resource management tools such as Univa Grid Engine (UGE) or Slurm to obtain application information and resource usage data. Additionally, it also uses a time-series database (e.g. InfluxDB) for data storage. MonSTer correlates applications to resource usage and reveals insightful knowledge without having additional overhead on the application and computing nodes. This paper presents the design and implementation of MonSTer, as well as experiences gained through real-world deployment on the 467-node Quanah cluster at Texas Tech University's High Performance Computing Center (HPCC) over the past year.},
	urldate = {2023-10-15},
	booktitle = {2020 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Li, Jie and Ali, Ghazanfar and Nguyen, Ngan and Hass, Jon and Sill, Alan and Dang, Tommy and Chen, Yong},
	month = sep,
	year = {2020},
	note = {ISSN: 2168-9253},
	pages = {119--129},
	file = {IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/FZ9V5HTR/Li et al. - 2020 - MonSTer An Out-of-the-Box Monitoring Tool for Hig.pdf:application/pdf},
}

@inproceedings{li_pims_2019,
	address = {New York, NY, USA},
	series = {{MEMSYS} '19},
	title = {{PIMS}: a lightweight processing-in-memory accelerator for stencil computations},
	isbn = {978-1-4503-7206-0},
	shorttitle = {{PIMS}},
	url = {https://doi.org/10.1145/3357526.3357550},
	doi = {10.1145/3357526.3357550},
	abstract = {Stencil computation is a classic computational kernel present in many high-performance scientific applications, like image processing and partial differential equation solvers (PDE). A stencil computation sweeps over a multi-dimensional grid and repeatedly updates values associated with points using the values from neighboring points. Stencil computations often employ large datasets that exceed cache capacity, leading to excessive accesses to the memory subsystem. As such, 3D stencil computations on large grid sizes are memory-bound. In this paper we present PIMS, an in-memory accelerator for stencil computations. PIMS, implemented in the logic layer of a 3D-stacked memory, exploits the high bandwidth provided by through-silicon vias to reduce redundant memory traffic. Our comprehensive evaluation using three different grid sizes with six categories of orders indicate that the proposed architecture reduces 48.25\% of data movement on average and obtains up to 65.55\% of bank conflict reduction.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the {International} {Symposium} on {Memory} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jie and Wang, Xi and Tumeo, Antonino and Williams, Brody and Leidel, John D. and Chen, Yong},
	month = sep,
	year = {2019},
	keywords = {high performance computing, hybrid memory cube, processing-in-memory, stencil computation},
	pages = {41--52},
}

@inproceedings{wang_mac_2019,
	address = {New York, NY, USA},
	series = {{ICPP} '19},
	title = {{MAC}: {Memory} {Access} {Coalescer} for {3D}-{Stacked} {Memory}},
	isbn = {978-1-4503-6295-5},
	shorttitle = {{MAC}},
	url = {https://doi.org/10.1145/3337821.3337867},
	doi = {10.1145/3337821.3337867},
	abstract = {Emerging data-intensive applications, such as graph analytics and data mining, exhibit irregular memory access patterns. Research has shown that with these memory-bound applications, traditional cache-based processor architectures, which exploit locality and regular patterns to mitigate the memory-wall issue, are inefficient. Meantime, novel 3D-stacked memory devices, such as Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM), promise significant increases in bandwidth that appear extremely appealing for memory-bound applications. However, conventional memory interfaces designed for cache-based architectures and JEDEC DDR devices fit poorly with the 3D-stacked memory, which leads to significant under-utilization of the promised high bandwidth. As a response to these issues, in this paper we propose MAC (Memory Access Coalescer), a coalescing unit for the 3D-stacked memory. We discuss the design and implementation of MAC, in the context of a custom designed cache-less architecture targeted at data-intensive, irregular applications. Through a custom simulation infrastructure based on the RISC-V toolchain, we show that MAC achieves a coalescing efficiency of 52.85\% on average. It improves the performance of the memory system by 60.73\% on average for a large set of irregular workloads.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 48th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Xi and Tumeo, Antonino and Leidel, John D. and Li, Jie and Chen, Yong},
	month = aug,
	year = {2019},
	keywords = {3D-Stacked Memory, Irregular Applications, Memory Coalescing},
	pages = {1--10},
}

@inproceedings{nguyen_radarviewer_2020,
	address = {New York, NY, USA},
	series = {{PEARC} '20},
	title = {{RadarViewer} : {Visualizing} the dynamics of multivariate data},
	isbn = {978-1-4503-6689-2},
	shorttitle = {{RadarViewer}},
	url = {https://doi.org/10.1145/3311790.3404538},
	doi = {10.1145/3311790.3404538},
	abstract = {This showcase presents a visual approach based on clustering and superimposing to construct a high-level overview of sequential event data while balancing the amount of information and the cardinality in it. We also implement an interactive prototype, called RadarViewer , that allows domain analysts to simultaneously analyze sequence clustering, extract useful distribution patterns, drill multiple levels-of-detail to accelerate the analysis. The RadarViewer is demonstrated through case studies with real-world temporal datasets of different sizes.},
	urldate = {2023-10-14},
	booktitle = {Practice and {Experience} in {Advanced} {Research} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Ngan and Hass, Jon and Chen, Yong and Li, Jie and Sill, Alan and Dang, Tommy},
	month = jul,
	year = {2020},
	keywords = {multivariate data analysis, Radar chart, time-series visualization},
	pages = {555--556},
}

@article{wang_ham_2021,
	title = {{HAM}: {Hotspot}-{Aware} {Manager} for {Improving} {Communications} {With} {3D}-{Stacked} {Memory}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {{HAM}},
	url = {https://ieeexplore.ieee.org/abstract/document/9381710},
	doi = {10.1109/TC.2021.3066982},
	abstract = {Emerging High-Performance Computing (HPC) workloads, such as graph analytics, machine learning, and big data science, are data-intensive. Data-intensive workloads usually present fine-grained memory accesses with limited or no data locality, and thus incur frequent cache misses and low utilization of memory bandwidth. 3D-stacked memory devices such as Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM) can provide significantly higher bandwidth than conventional memory modules. However, the traditional interfaces and optimization methods for JEDEC DDR devices do not allow to fully exploit the potential performance of 3D-stacked memory with the massive amount of irregular memory accesses of data-intensive applications. In this article, we propose a novel Hotspot-Aware Manager (HAM) infrastructure for 3D-stacked memory devices capable of optimizing memory access streams via request aggregation, hotspot detection, and in-memory prefetching. We present the HAM design and implementation, and simulate it on a system using RISC-V embedded cores with attached HMC devices. We extensively evaluate HAM with over 12 benchmarks and applications representing diverse irregular memory access patterns. The results show that, on average, HAM reduces redundant requests by 37.51 percent and increases the prefetch buffer hit rate by 4.2 times, compared to a baseline streaming prefetcher. On the selected benchmark set, HAM provides performance gains of 21.81 percent in average (up to 34.28 percent), and power savings of 35.07 percent over a standard 3D-stacked memory.},
	number = {6},
	urldate = {2023-10-15},
	journal = {IEEE Transactions on Computers},
	author = {Wang, Xi and Tumeo, Antonino and Leidel, John D. and Li, Jie and Chen, Yong},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	pages = {833--848},
	file = {IEEE Xplore Abstract Record:/Users/jieli/Zotero/storage/ZGPREHJV/9381710.html:text/html;IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/G57KYBAL/Wang et al. - 2021 - HAM Hotspot-Aware Manager for Improving Communica.pdf:application/pdf},
}

@book{dang_gap_2021,
	title = {The {Gap} between {Visualization} {Research} and {Visualization} {Software} in {High}-{Performance} {Computing} {Center}},
	isbn = {978-3-03868-149-6},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/visgap20211089},
	abstract = {Visualizing and monitoring high-performance computing centers is a daunting task due to the systems' complex and dynamic nature. Moreover, different users may have different requirements and needs. For example, computer scientists carry out data analysis as batch jobs using various models, configurations, and parameters, and they often need to manage jobs. System administrators need to monitor and manage the system constantly. In this paper, we discuss the gap between visual monitoring research and practical applicability. We will start with the general requirements for managing high-performance computing centers and then share the experiences working with academic and industrial experts in this domain.},
	language = {en},
	urldate = {2023-10-15},
	publisher = {The Eurographics Association},
	author = {Dang, Tommy and Nguyen, Ngan and Hass, Jon and Li, Jie and Chen, Yong and Sill, Alan},
	year = {2021},
	doi = {10.2312/visgap.20211089},
	note = {Accepted: 2021-06-12T11:28:46Z},
	file = {Full Text PDF:/Users/jieli/Zotero/storage/IPAAKPY6/Dang et al. - 2021 - The Gap between Visualization Research and Visuali.pdf:application/pdf},
}

@inproceedings{dang_jobviewer_2022,
	title = {{JobViewer}: {Graph}-based {Visualization} for {Monitoring} {High}-{Performance} {Computing} {System}},
	shorttitle = {{JobViewer}},
	url = {https://ieeexplore.ieee.org/document/10062112},
	doi = {10.1109/BDCAT56447.2022.00021},
	abstract = {Visualization aims to strengthen data exploration and analysis, especially for complex and high-dimensional data. High-performance computing (HPC) systems are typically large and complicated instruments that generate massive performance and operation time series. Monitoring HPC systems’ performance is a daunting task for HPC admins and researchers due to their dynamic natures. This work proposes a visual design using the bipartite graph’s idea to visualize HPC clusters’ structure, metrics, and job scheduling data. We built a web-based prototype, called JobViewer, that integrates advanced methods in visualization and human-computer interaction (HCI) to demonstrate the benefits of visualization in real-time monitoring HPC centers. We also showed real use cases and a user study to validate the efficiency and highlight the current approach’s drawbacks.},
	urldate = {2023-10-15},
	booktitle = {2022 {IEEE}/{ACM} {International} {Conference} on {Big} {Data} {Computing}, {Applications} and {Technologies} ({BDCAT})},
	author = {Dang, Tommy and Nguyen, Ngan V.T. and Li, Jie and Sill, Alan and Hass, Jon and Chen, Yong},
	month = dec,
	year = {2022},
	pages = {110--119},
	file = {IEEE Xplore Abstract Record:/Users/jieli/Zotero/storage/5J7ZQSI2/10062112.html:text/html;IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/Q8MG9Z6R/Dang et al. - 2022 - JobViewer Graph-based Visualization for Monitorin.pdf:application/pdf},
}

@misc{li_arcode_2023,
	title = {{ARcode}: {HPC} {Application} {Recognition} {Through} {Image}-encoded {Monitoring} {Data}},
	shorttitle = {{ARcode}},
	url = {http://arxiv.org/abs/2301.08612},
	doi = {10.48550/arXiv.2301.08612},
	abstract = {Knowing HPC applications of jobs and analyzing their performance behavior play important roles in system management and optimizations. The existing approaches detect and identify HPC applications through machine learning models. However, these approaches rely heavily on the manually extracted features from resource utilization data to achieve high prediction accuracy. In this study, we propose an innovative application recognition method, ARcode, which encodes job monitoring data into images and leverages the automatic feature learning capability of convolutional neural networks to detect and identify applications. Our extensive evaluations based on the dataset collected from a large-scale production HPC system show that ARcode outperforms the state-of-the-art methodology by up to 18.87\% in terms of accuracy at high confidence thresholds. For some specific applications (BerkeleyGW and e3sm), ARcode outperforms by over 20\% at a confidence threshold of 0.8.},
	urldate = {2023-10-15},
	publisher = {arXiv},
	author = {Li, Jie and Cook, Brandon and Chen, Yong},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08612 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/jieli/Zotero/storage/JXN83SXW/Li et al. - 2023 - ARcode HPC Application Recognition Through Image-.pdf:application/pdf;arXiv.org Snapshot:/Users/jieli/Zotero/storage/YK48DY3K/2301.html:text/html},
}

@inproceedings{li_analyzing_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Analyzing {Resource} {Utilization} in an {HPC} {System}: {A} {Case} {Study} of {NERSC}’s {Perlmutter}},
	isbn = {978-3-031-32041-5},
	shorttitle = {Analyzing {Resource} {Utilization} in an {HPC} {System}},
	doi = {10.1007/978-3-031-32041-5_16},
	abstract = {Resource demands of HPC applications vary significantly. However, it is common for HPC systems to primarily assign resources on a per-node basis to prevent interference from co-located workloads. This gap between the coarse-grained resource allocation and the varying resource demands can lead to HPC resources being not fully utilized. In this study, we analyze the resource usage and application behavior of NERSC’s Perlmutter, a state-of-the-art open-science HPC system with both CPU-only and GPU-accelerated nodes. Our one-month usage analysis reveals that CPUs are commonly not fully utilized, especially for GPU-enabled jobs. Also, around 64\% of both CPU and GPU-enabled jobs used 50\% or less of the available host memory capacity. Additionally, about 50\% of GPU-enabled jobs used up to 25\% of the GPU memory, and the memory capacity was not fully utilized in some ways for all jobs. While our study comes early in Perlmutter’s lifetime thus policies and application workload may change, it provides valuable insights on performance characterization, application behavior, and motivates systems with more fine-grain resource allocation.},
	language = {en},
	booktitle = {High {Performance} {Computing}},
	publisher = {Springer Nature Switzerland},
	author = {Li, Jie and Michelogiannakis, George and Cook, Brandon and Cooray, Dulanya and Chen, Yong},
	editor = {Bhatele, Abhinav and Hammond, Jeff and Baboulin, Marc and Kruse, Carola},
	year = {2023},
	keywords = {Disaggregated Memory, GPU Utilization, HPC, Large-scale Characterization, Memory System, Resource Utilization},
	pages = {297--316},
	file = {Full Text PDF:/Users/jieli/Zotero/storage/WKPEYKIY/Li et al. - 2023 - Analyzing Resource Utilization in an HPC System A.pdf:application/pdf},
}

@inproceedings{li_workload_2023,
	title = {Workload {Failure} {Prediction} for {Data} {Centers}},
	url = {https://ieeexplore.ieee.org/document/10254988},
	doi = {10.1109/CLOUD60044.2023.00064},
	abstract = {Failed workloads that consumed significant computational resources in time and space affect the efficiency of HPC data centers significantly and thus limit the amount of scientific work that can be achieved. While the computational power has increased significantly over the years, detection and prediction of workload failures have lagged far behind and will become increasingly critical as the system scale and complexity further increase. In this study, we analyze workload traces collected from a production cluster and train machine learning models on a large amount of data sets to predict workload failures. Our prediction models consist of a queue-time model that estimates the probability of workload failures before execution and a runtime model that predicts failures at runtime. Evaluation results show that the queue-time model and runtime model can predict workload failures with a maximum precision score of 90.61\% and 97.75\%, respectively. By integrating the runtime model with the job scheduler, it helps reduce CPU time, and memory usage by up to 16.7\% and 14.53\%, respectively.},
	urldate = {2023-10-15},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Li, Jie and Wang, Rui and Ali, Ghazanfar and Dang, Tommy and Sill, Alan and Chen, Yong},
	month = jul,
	year = {2023},
	note = {ISSN: 2159-6190},
	pages = {479--485},
	file = {IEEE Xplore Abstract Record:/Users/jieli/Zotero/storage/5KIUZ657/10254988.html:text/html;IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/N6Q6JU98/Li et al. - 2023 - Workload Failure Prediction for Data Centers.pdf:application/pdf},
}

@inproceedings{caon_effective_2023,
	title = {Effective {Management} of {Time} {Series} {Data}},
	url = {https://ieeexplore.ieee.org/document/10254974},
	doi = {10.1109/CLOUD60044.2023.00055},
	abstract = {Cloud computing systems, consisting of numerous nodes and components, require constant monitoring to satisfy the Quality-of-Service (QoS), making the management of large-scale time series data challenging. To address this issue, age threshold retention policies have been implemented to remove historical data, but this eliminates valuable information from older periods. In this paper, we proposed an alternative approach that applies time series deduplication with metric-based tolerance to discard readings that stabilize within a calculated tolerance window. This approach can reduce the data volume by 70.38\% on average. Once the data-reduced interval is queried, the readings can be reconstructed to retrieve the original granularity with low query runtime overhead and a Mean Absolute Percentage Error of 0.74\%.},
	urldate = {2023-10-15},
	booktitle = {2023 {IEEE} 16th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Caon, Cristiano E. and Li, Jie and Chen, Yong},
	month = jul,
	year = {2023},
	note = {ISSN: 2159-6190},
	pages = {408--414},
	file = {IEEE Xplore Abstract Record:/Users/jieli/Zotero/storage/5WCR8MS8/10254974.html:text/html;IEEE Xplore Full Text PDF:/Users/jieli/Zotero/storage/6U48CDRU/Caon et al. - 2023 - Effective Management of Time Series Data.pdf:application/pdf},
}
